grads <- calc_grads(plist = parlist, hlay = hlay
, yhat = yhat[curBat], curBat = curBat
, droplist = droplist, dropinp = dropinp, treatment = treatment)
#Pad the gradients with zeros to scale it back to the original size
if (dropout_hidden < 1){
for (i in 1:(length(grads)-1)){
gr <- matrix(rep(0, length(curBat)*length(droplist[[i]])), nrow = length(curBat))
gr[,droplist[[i]]] <- grads[[i]]
grads[[i]] <- gr
}
}
#Calculate updates to parameters based on gradients and learning rates
if (RMSprop == TRUE){
newG2 <- foreach(i = 1:(length(hlayers)+1)) %do% {
if (i == 1){D <- X[curBat,]} else {D <- hlayers[[i-1]][curBat,]}
if (bias_hlayers == TRUE & i != length(hlayers)+1){D <- cbind(1, D)}
.1*(t(D) %*% grads[[i]])^2
}
oldG2 <- lapply(G2, function(x){.9*x})
G2 <- mapply('+', newG2, oldG2)
uB <- LR/sqrt(G2[[length(G2)]]+1e-10) *
t(t(grads[[length(grads)]]) %*% hlayers[[length(hlayers)]][curBat,]) +
LR*as.matrix(2*lam*c(parlist$beta_param*parapen#penalty/weight decay...
, 0*parlist$beta_treatment, parlist$beta)
)#Treatment is always unpenalized
updates$beta_param <- uB[1:length(parlist$beta_param)]
updates$beta <- uB[grepl('nodes', rownames(uB))]
if (!is.null(treatment)){
updates$beta_treatment <- uB[rownames(uB) == 'treatment']
}
for(i in nlayers:1){
if(i == 1){lay = X[curBat,]} else {lay = hlayers[[i-1]][curBat,]}
if(bias_hlayers == TRUE){lay <- cbind(1,lay)}
updates[[i]] <- LR/sqrt(G2[[i]]+1e-10) * t(t(grads[[i]]) %*% lay) + LR*t(2 * lam * t(parlist[[i]]))
}
} else { #if RMSprop == FALSE
uB <- LR * t(t(grads[[length(grads)]]) %*% hlayers[[length(hlayers)]][curBat,] +
2*lam*c(parlist$beta_param*parapen, 0*parlist$beta_treatment, parlist$beta,))
updates$beta_param <- uB[1:length(parlist$beta_param)]
updates$beta <- uB[grepl('nodes', rownames(uB))]
if (!is.null(treatment)){
updates$beta_treatment <- uB[rownames(uB) == 'treatment']
}
for(i in nlayers:1){
if(i == 1){lay = X[curBat,]} else {lay = hlayers[[i-1]][curBat,]}
if(bias_hlayers == TRUE){lay <- cbind(1,lay)}
updates[[i]] <- t(LR * t(grads[[i]]) %*% lay + 2 * lam * t(parlist[[i]]))
}
}
#Update parameters from update list
parlist <- as.relistable(mapply('-', parlist, updates))
pl <- unlist(parlist)
#Update hidden layers
hlayers <- calc_hlayers(parlist)
#OLS trick!
#mid
if (OLStrick == TRUE & OLStrick_iter == 1) {
parlist <- OLStrick_function(parlist = parlist, hidden_layers = hlayers, y = y
, fe_var = fe_var, lam = lam, parapen = parapen, treatment = treatment
)
pl <- unlist(parlist)
}
#Ols_trick_every_#_iterations
if (OLStrick == TRUE & iter %% OLStrick_iter == 0 & iter > 1){
parlist <- OLStrick_function(parlist = parlist, hidden_layers = hlayers, y = y
, fe_var = fe_var, lam = lam, parapen = parapen, treatment = treatment
)
pl <- unlist(parlist)
}
#update yhat
yhat <- getYhat(pl, attr(pl, 'skeleton'), hlay = hlayers)
mse <- mean((y-yhat)^2)
msevec <- append(msevec, mse)
loss <- mse + lam*sum(c(parlist$beta_param*parapen
, 0*parlist$beta_treatment, parlist$beta
, unlist(parlist[!grepl('beta', names(parlist))]))^2)
lossvec <- append(lossvec, loss)
}
#Finished epoch.  Assess whether MSE has increased and revert if so
mse <- mean((y-yhat)^2)
loss <- mse + lam*sum(c(parlist$beta_param*parapen
, 0*parlist$beta_treatment, parlist$beta
, unlist(parlist[!grepl('beta', names(parlist))]))^2
)
#If loss increases...
if (oldpar$loss <= loss){
parlist <- oldpar$parlist
updates <- oldpar$updates
G2 <- oldpar$G2
hlayers <- oldpar$hlayers
grads <- oldpar$grads
yhat <- oldpar$yhat
mse <- oldpar$mse
mseold <- oldpar$mseold
stopcounter <- stopcounter + 1
loss <- oldpar$loss
msevec <- oldpar$msevec
lossvec <- oldpar$lossvec
LR <- LR/2
if(verbose == TRUE){
print(paste0("Loss increased.  halving LR.  Stopcounter now at ", stopcounter))
}
} else {
LRvec[iter+1] <- LR <- LR*gravity      #gravity...
D <- oldpar$loss - loss
if (D<convtol){
stopcounter <- stopcounter +1
if(verbose == TRUE){print(paste('slowing!  Stopcounter now at ', stopcounter))}
}else{
stopcounter <-0
}
if (verbose == TRUE & iter %% report_interval == 0){
writeLines(paste0(
"*******************************************\n"
, 'Lambda = ',lam, "\n"
, "Hidden units -> ",paste(hidden_units, collapse = ' '), "\n"
, " Batch size is ", batchsize, " \n"
, " Completed ", iter, " epochs. \n"
, " Completed ", bat, " batches in current epoch. \n"
, "mse is ",mse, "\n"
, "last mse was ", oldpar$mse, "\n"
, "difference is ", oldpar$mse - mse, "\n"
, "loss is ",loss, "\n"
, "last loss was ", oldpar$loss, "\n"
, "difference is ", oldpar$loss - loss, "\n"
, "*******************************************\n"
))
if (para_plot == TRUE){#additional plots if plotting parameter evolution
par(mfrow = c(ceiling(length(parlist)/2)+3,2))
} else {
par(mfrow = c(3,2))
}
plot(y, yhat, col = rgb(1,0,0,.5), pch = 19, main = 'in-sample performance')
abline(0,1)
plot(LRvec, type = 'b', main = 'learning rate history')
plot(msevec, type = 'l', main = 'all epochs')
plot(msevec[(1+(iter)*max(batchid)):length(msevec)], type = 'l', ylab = 'mse', main = 'Current epoch')
plot(lossvec, type = 'l', main = 'all epochs')
plot(lossvec[(1+(iter)*max(batchid)):length(lossvec)], type = 'l', ylab = 'loss', main = 'Current epoch')
if (para_plot == TRUE){
#update para plot list
for (lay in 1:length(para_plot_list)){
x <- as.matrix(as.numeric(parlist[[lay]]))
l <- length(x)
if (l > 30){
q <- quantile(x, probs = seq(.05, .95, by = .1))
mu = mean(x)
para_plot_list[[lay]] <- cbind(para_plot_list[[lay]], c(q, mu = mu))
plot(para_plot_list[[lay]]['mu',], ylim = range(para_plot_list[[lay]])
, type = 'l', col = 'red', main = names(parlist)[[lay]], ylab = 'weights'
)
apply(para_plot_list[[lay]][-nrow(para_plot_list[[lay]]),], 1, lines, col = 'black')
abline(h = 0, lty = 2)
} else {
para_plot_list[[lay]] <- cbind(para_plot_list[[lay]], x)
plot(apply(para_plot_list[[lay]], 2, mean), ylim = range(para_plot_list[[lay]])
, type = 'l', col = 'red', main = names(parlist)[[lay]], ylab = 'weights'
)
apply(para_plot_list[[lay]], 1, lines, col = 'grey')
abline(h = 0, lty = 2)
}
}
}
}
}
iter <- iter+1
} #close the while loop
#If trained with dropout, weight the layers by expectations
if(dropout_hidden<1){
for (i in nlayers:1){
if (i == 1){
parlist[[i]] <- parlist[[i]] * dropout_input
} else {
parlist[[i]] <- parlist[[i]] * dropout_hidden
}
}
parlist$beta <- parlist$beta * dropout_hidden
if (OLStrick == TRUE){
parlist <- OLStrick_function(parlist = parlist, hidden_layers = hlayers, y = y
, fe_var = fe_var, lam = lam, parapen = parapen, treatment = treatment
)
}
#redo the hidden layers based on the new parlist
hlayers <- calc_hlayers(parlist)
yhat <- getYhat(unlist(parlist), hlay = hlayers)
}
conv <- (iter<maxit)#Did we get convergence?
if(is.null(fe_var)){
fe_output <- NULL
} else {
Zdm <- demeanlist(hlayers[[length(hlayers)]], list(fe_var))
fe <- (y-ydm) - as.matrix(hlayers[[length(hlayers)]]-Zdm) %*% as.matrix(c(
parlist$beta_param, parlist$beta_treatment
, parlist$beta
))
fe_output <- data.frame(fe_var, fe)
}
output <- list(yhat = yhat, parlist = parlist, hidden_layers = hlayers
, fe = fe_output, converged = conv, mse = mse, loss = loss, lam = lam, time_var = time_var
, X = X, y = y, param = param, fe_var = fe_var, hidden_units = hidden_units, maxit = maxit
, used_bias = bias_hlayers, final_improvement = D, msevec = msevec, RMSprop = RMSprop, convtol = convtol
, grads = grads, activation = activation, parapen = parapen, doscale = doscale, treatment = treatment
, batchsize = batchsize, initialization = initialization)
return(output)
}
OLStrick_function <- function(parlist, hidden_layers, y, fe_var, lam, parapen, treatment){
#
# parlist <- pnn$parlist
# hidden_layers <- pnn$hidden_layers
# y = pnn$y
# fe_var <- pnn$fe_var
# lam <- pnn$lam
# parapen <- pnn$parapen
# treatment = pnn$treatment
# mse <- pnn$mse
constraint <- sum(c(parlist$beta_param*parapen, parlist$beta)^2)
#getting implicit regressors depending on whether regression is panel
if (is.null(treatment)){
if (!is.null(fe_var)){
Zdm <- demeanlist(hidden_layers[[length(hidden_layers)]], list(fe_var))
targ <- demeanlist(y, list(fe_var))
} else {
Zdm <- hidden_layers[[length(hidden_layers)]]
targ <- y
}
} else {#HTE case with fe's
if (!is.null(fe_var)){
V <- hidden_layers[[length(hidden_layers)]]
ints <- sweep(V[,grepl('nodes', colnames(V))], MARGIN = 1, STATS = treatment, "*")
colnames(ints) <- paste0(colnames(ints),"_int")
dmat <- data.frame(V, ints)
Zdm <- as.matrix(demeanlist(dmat, list(fe_var)))
targ <- demeanlist(y, list(fe_var))
} else {#HTE case
V <- hidden_layers[[length(hidden_layers)]]
ints <- sweep(V[,grepl('nodes', colnames(V))], MARGIN = 1, STATS = treatment, "*")
colnames(ints) <- paste0(colnames(ints),"_int")
dmat <- data.frame(V, ints)
targ <- y
}
}
#setup_penalty_vector
D <- rep(1, ncol(Zdm)) #D=numeric
if (is.null(fe_var)){
pp <- c(0, parapen) #never penalize the intercept
} else {
pp <- parapen #parapen
}
D[1:length(pp)] <- D[1:length(pp)]*pp #incorporate parapen into diagonal of covmat
#set to zero -> treatment columns and main effects of HTE models #treatments and ME's are not penalized
#onlt_penalize_the_interacted_nodes
if (!is.null(treatment)){
tozero <- grepl('treatment', colnames(Zdm)) | (grepl('nodes', colnames(Zdm)) & !grepl('int', colnames(Zdm)))
D[tozero] <- 0
}
# if (lam != 0){
#   #implicit_lambda_function
#   f <- function(lam){
#     bi <- glmnet(y = targ, x = Zdm, lambda = lam, alpha = 0, intercept = FALSE, penalty.factor = D, standardize = FALSE)
#     bi <- as.matrix(coef(bi))[-1,]
#     bi1 <- (crossprod(bi*D) - constraint)^2
#     return(bi1)
#   }
if (lam != 0){
#implicit_lambda_function
f <- function(lam){
#minimize _mse_given_constraint_over_betas_and_lam
bi <- glmnet(y = targ, x = Zdm, lambda = lam, alpha = 0, intercept = FALSE, penalty.factor = D, standardize = FALSE)
bi <- as.matrix(coef(bi))[-1,]
bi <- (crossprod(bi*D) - constraint)^2
bi <- matrix(rep(bi, (ncol(Zdm)*(nrow(Zdm)))), ncol(Zdm)) #turn_bi_to_scalar_for_conformibility
bi <- (Zdm %*% bi)
bi <- as.matrix(bi[1,])
bi1 <- mean((targ - bi)^2) # targ_==_y
#mse_1 <- mean((targ - Zdm %*% c(parlist$beta_param, parlist$beta))^2
return(bi1)
}
#optimize it
o <- optim(par = lam, fn = f, method = 'Brent', lower = lam, upper = 1e9) #fn_=_function_to_be_minimized
#new_lambda
newlam <- o$par
} else {
newlam <- 0 # lam == 0
}
#New_top_level_params
br <- glmnet(y = targ, x = Zdm, lambda = newlam, alpha = 0, intercept = FALSE, penalty.factor = D, standardize = FALSE)
b <- as.matrix(coef(br))[-1,,drop = FALSE]
parlist$beta_param <- b[grepl('param', rownames(b))]
parlist$beta <- b[grepl('nodes', rownames(b))]
return(parlist)
}
# pnn$mse
# mean((targ - Zdm %*% b)^2)
#mean((targ - Zdm %*% c(parlist$beta_param, parlist$beta))^2)
#mean((targ - Zdm %*% b)^2)
OLStrick_function
pnn <- panelNNET.est(y = y, X = z_1
, hidden_units = c(10:5)
, fe_var = id
, maxit = 200
, lam = .01
, time_var = NULL
, param = Pterm
, parapen = c(0,0)
, parlist = NULL #parlist
, verbose = TRUE
, para_plot = FALSE
, report_interval = 2
, gravity = 1.01
, convtol = 1e-8
, batchsize = nrow(z_1)
, bias_hlayers = TRUE
, RMSprop = TRUE
, start_LR = .001
, activation = 'lrelu'
, doscale = TRUE
, treatment = T_2
, maxstopcounter = 10
, OLStrick = TRUE
, OLStrick_iter = 15
, initialization = 'enforce_normalization'
, dropout_hidden = 1
, dropout_input = 1)
pnn <- panelNNET.est(y = y, X = z_1
, hidden_units = c(10:5)
, fe_var = id
, maxit = 200
, lam = .01
, time_var = NULL
, param = Pterm
, parapen = c(0,0)
, parlist = NULL #parlist
, verbose = TRUE
, para_plot = FALSE
, report_interval = 2
, gravity = 1.01
, convtol = 1e-8
, batchsize = nrow(z_1)
, bias_hlayers = TRUE
, RMSprop = TRUE
, start_LR = .001
, activation = 'lrelu'
, doscale = TRUE
, treatment = T_2
, maxstopcounter = 10
, OLStrick = TRUE
, OLStrick_iter = 15
, initialization = 'enforce_normalization'
, dropout_hidden = 1
, dropout_input = 1)
pnn <- panelNNET.est(y = y, X = z_1
, hidden_units = c(10:5)
, fe_var = id
, maxit = 200
, lam = .01
, time_var = NULL
, param = Pterm
, parapen = c(0,0)
, parlist = NULL #parlist
, verbose = TRUE
, para_plot = FALSE
, report_interval = 2
, gravity = 1.01
, convtol = 1e-8
, batchsize = nrow(z_1)
, bias_hlayers = TRUE
, RMSprop = TRUE
, start_LR = .001
, activation = 'lrelu'
, doscale = TRUE
, treatment = T_2
, maxstopcounter = 10
, OLStrick = TRUE
, OLStrick_iter = 15
, initialization = 'enforce_normalization'
, dropout_hidden = 1
, dropout_input = 1)
pnn <- panelNNET.est(y = y, X = z_1
, hidden_units = c(10:5)
, fe_var = id
, maxit = 200
, lam = .01
, time_var = NULL
, param = Pterm
, parapen = c(0,0)
, parlist = NULL #parlist
, verbose = TRUE
, para_plot = FALSE
, report_interval = 2
, gravity = 1.01
, convtol = 1e-8
, batchsize = nrow(z_1)
, bias_hlayers = TRUE
, RMSprop = TRUE
, start_LR = .001
, activation = 'lrelu'
, doscale = TRUE
, treatment = T_2
, maxstopcounter = 10
, OLStrick = TRUE
, OLStrick_iter = 15
, initialization = 'enforce_normalization'
, dropout_hidden = 1
, dropout_input = 1)
pnn <- panelNNET.est(y = y, X = z_1
, hidden_units = c(10:5)
, fe_var = id
, maxit = 200
, lam = .01
, time_var = NULL
, param = Pterm
, parapen = c(0,0)
, parlist = NULL #parlist
, verbose = TRUE
, para_plot = FALSE
, report_interval = 2
, gravity = 1.01
, convtol = 1e-8
, batchsize = nrow(z_1)
, bias_hlayers = TRUE
, RMSprop = TRUE
, start_LR = .001
, activation = 'lrelu'
, doscale = TRUE
, treatment = T_2
, maxstopcounter = 10
, OLStrick = TRUE
, OLStrick_iter = 15
, initialization = 'enforce_normalization'
, dropout_hidden = 1
, dropout_input = 1)
pnn <- panelNNET.est(y = y, X = z_1
, hidden_units = c(10:5)
, fe_var = id
, maxit = 200
, lam = .01
, time_var = NULL
, param = Pterm
, parapen = c(0,0)
, parlist = NULL #parlist
, verbose = TRUE
, para_plot = FALSE
, report_interval = 2
, gravity = 1.01
, convtol = 1e-8
, batchsize = nrow(z_1)
, bias_hlayers = TRUE
, RMSprop = TRUE
, start_LR = .001
, activation = 'lrelu'
, doscale = TRUE
, treatment = T_2
, maxstopcounter = 10
, OLStrick = TRUE
, OLStrick_iter = 15
, initialization = 'enforce_normalization'
, dropout_hidden = 1
, dropout_input = 1)
pnn <- panelNNET.est(y = y, X = z_1
, hidden_units = c(10:5)
, fe_var = id
, maxit = 200
, lam = .01
, time_var = NULL
, param = Pterm
, parapen = c(0,0)
, parlist = NULL #parlist
, verbose = TRUE
, para_plot = FALSE
, report_interval = 2
, gravity = 1.01
, convtol = 1e-8
, batchsize = nrow(z_1)
, bias_hlayers = TRUE
, RMSprop = TRUE
, start_LR = .001
, activation = 'lrelu'
, doscale = TRUE
, treatment = T_2
, maxstopcounter = 10
, OLStrick = TRUE
, OLStrick_iter = 15
, initialization = 'enforce_normalization'
, dropout_hidden = 1
, dropout_input = 1)
pnn <- panelNNET.est(y = y, X = z_1
, hidden_units = c(10:5)
, fe_var = id
, maxit = 200
, lam = .01
, time_var = NULL
, param = Pterm
, parapen = c(0,0)
, parlist = NULL #parlist
, verbose = TRUE
, para_plot = FALSE
, report_interval = 2
, gravity = 1.01
, convtol = 1e-8
, batchsize = nrow(z_1)
, bias_hlayers = TRUE
, RMSprop = TRUE
, start_LR = .001
, activation = 'lrelu'
, doscale = TRUE
, treatment = T_2
, maxstopcounter = 10
, OLStrick = TRUE
, OLStrick_iter = 15
, initialization = 'enforce_normalization'
, dropout_hidden = 1
, dropout_input = 1)
